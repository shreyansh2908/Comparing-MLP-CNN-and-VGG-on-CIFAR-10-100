{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " # Comparing MLP, CNN, and VGG on CIFAR-10/100\n",
        " The CIFAR-10 and CIFAR-100 datasets are benchmarks for image classification tasks in\n",
        " computer vision. In this exercise, you will explore how different neural network architectures\n",
        " perform on these image datasets.\n",
        " Tasks\n",
        " 1. Multilayer Perceptron (MLP):\n",
        " ○ Develop a Multilayer Perceptron (MLP) model with an appropriate architecture (number\n",
        " of layers, hidden units, etc.).\n",
        " ○ Train the MLP model on the CIFAR-10 dataset.\n",
        " ○ Evaluate its performance on the test set using metrics like accuracy and loss. Analyze\n",
        " the results.\n",
        " 2. Convolutional Neural Network (CNN):\n",
        " ○ Design asimple Convolutional Neural Network (CNN) architecture with convolutional\n",
        " layers, pooling layers, and fully connected layers.\n",
        " ○ Train the CNN model on the CIFAR-10 dataset.\n",
        " ○ Evaluate its performance on the test set using the same metrics as the MLP. Critically\n",
        " compare CNN's performance to the MLP's.\n",
        " 3. Transfer Learning with VGG:\n",
        " ○ Utilize a pre-trained VGG model (such as VGG16 or VGG19) available in common deep\n",
        " learning libraries.\n",
        " ○ Adapt the pre-trained VGG model for the CIFAR-10 or CIFAR-100 classification task.\n",
        " ○ Evaluate its performance on the test set. Analyze the benefits of transfer learning\n",
        " compared to your MLP and CNN models trained from scratch.\n",
        " Analysis and Discussion\n",
        " ● Comparethe test set accuracy and loss across the MLP, CNN, and the VGG-based model.\n",
        " ● Discuss the reasons behind the differences in performance. Explain how CNNs leverage the\n",
        " spatial structure of images for better feature extraction compared to MLPs.\n",
        " ● Ifyou used the VGG model, elaborate on how transfer learning helped improve performance\n",
        " or reduce training time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i43p_b4crGO7",
        "outputId": "74e4fb1e-3f18-4527-a365-43804ca80bb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 43271123.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "\n",
        "# Setting up transformations and loading the CIFAR-10 dataset\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jflU9GFdjMW9"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Function to train and evaluate a model\n",
        "def train_and_evaluate(model, trainloader, testloader, optimizer, criterion, epochs=10, model_name=\"Model\"):\n",
        "    print(f\"Training {model_name}...\")\n",
        "    model.to(device)\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f\"[{epoch + 1}] loss: {running_loss / len(trainloader)}\")\n",
        "    print(f\"Finished Training {model_name}\")\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f'Accuracy of {model_name} on the 10000 test images: {100 * correct // total}%')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9AlPMiHrZh6",
        "outputId": "dca50ec9-1acf-4dee-c04b-9509107df32d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training MLP...\n",
            "[1] loss: 2.047630117677362\n",
            "[2] loss: 1.7593329147914487\n",
            "[3] loss: 1.640869938351614\n",
            "[4] loss: 1.5647027582463706\n",
            "[5] loss: 1.5039437718098732\n",
            "[6] loss: 1.4514864552051514\n",
            "[7] loss: 1.4026413323629239\n",
            "[8] loss: 1.3587142399814733\n",
            "[9] loss: 1.3162025050129122\n",
            "[10] loss: 1.27663499848617\n",
            "Finished Training MLP\n",
            "Accuracy of MLP on the 10000 test images: 52%\n"
          ]
        }
      ],
      "source": [
        "# MLP Model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(3*32*32, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 3*32*32)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "mlp = MLP()\n",
        "mlp_optimizer = optim.SGD(mlp.parameters(), lr=0.001, momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training and Evaluating MLP\n",
        "train_and_evaluate(mlp, trainloader, testloader, mlp_optimizer, criterion, epochs=10, model_name=\"MLP\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_T72y12YrUTD",
        "outputId": "382acbdf-9d5b-430b-8eb4-d1410361231b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training CNN...\n",
            "[1] loss: 2.2956380139836265\n",
            "[2] loss: 2.11522898984992\n",
            "[3] loss: 1.8761049963324272\n",
            "[4] loss: 1.71936155493607\n",
            "[5] loss: 1.6015472678882081\n",
            "[6] loss: 1.520629839061776\n",
            "[7] loss: 1.4602648010644157\n",
            "[8] loss: 1.4136011855071768\n",
            "[9] loss: 1.3731686511003147\n",
            "[10] loss: 1.3381310936130222\n",
            "Finished Training CNN\n",
            "Accuracy of CNN on the 10000 test images: 51%\n"
          ]
        }
      ],
      "source": [
        "# CNN Model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "cnn = CNN()\n",
        "cnn_optimizer = optim.SGD(cnn.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Training and Evaluating CNN\n",
        "train_and_evaluate(cnn, trainloader, testloader, cnn_optimizer, criterion, epochs=10, model_name=\"CNN\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wr64aYAONFj-",
        "outputId": "cf9a0174-6295-4ae4-e299-f781d96a18c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training EnhancedCNN...\n",
            "[1] loss: 1.5195838320438209\n",
            "[2] loss: 1.1410263409395047\n",
            "[3] loss: 0.9849245687732306\n",
            "[4] loss: 0.8917985618724238\n",
            "[5] loss: 0.819718127322319\n",
            "[6] loss: 0.7624441480164028\n",
            "[7] loss: 0.7143806425278144\n",
            "[8] loss: 0.673169952066963\n",
            "[9] loss: 0.6303996303502251\n",
            "[10] loss: 0.5952669923262828\n",
            "Finished Training EnhancedCNN\n",
            "Accuracy of EnhancedCNN on the 10000 test images: 74%\n"
          ]
        }
      ],
      "source": [
        "# CNN Model\n",
        "class EnhancedCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EnhancedCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.batchnorm1 = nn.BatchNorm2d(32)\n",
        "        self.batchnorm2 = nn.BatchNorm2d(64)\n",
        "        self.batchnorm3 = nn.BatchNorm2d(128)\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.batchnorm1(self.conv1(x))))\n",
        "        x = self.pool(torch.relu(self.batchnorm2(self.conv2(x))))\n",
        "        x = self.pool(torch.relu(self.batchnorm3(self.conv3(x))))\n",
        "        x = x.view(-1, 128 * 4 * 4)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Instantiate the model, optimizer, and loss function\n",
        "enhanced_cnn = EnhancedCNN().to(device)\n",
        "optimizer = optim.SGD(enhanced_cnn.parameters(), lr=0.001, momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Call to train and evaluate the enhanced model\n",
        "train_and_evaluate(enhanced_cnn, trainloader, testloader, optimizer, criterion, epochs=10, model_name=\"EnhancedCNN\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "o5zXpAmHOsva",
        "outputId": "fbd488b7-cc84-40c3-a0bf-fb37dbca7530"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x1000 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "def visualize_filters(model):\n",
        "    model_weights = model.state_dict()\n",
        "    conv1_weights = model_weights['conv1.weight']\n",
        "    # Normalize the weights for visualization purposes\n",
        "    min_w = torch.min(conv1_weights)\n",
        "    max_w = torch.max(conv1_weights)\n",
        "    conv1_weights = (conv1_weights - min_w) / (max_w - min_w)\n",
        "    # Assuming conv1_weights shape is (out_channels, in_channels, H, W)\n",
        "    # Create a grid of images\n",
        "    grid = torchvision.utils.make_grid(conv1_weights, nrow=8, padding=1, normalize=True)\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    # Move the grid to CPU and then convert to numpy for visualization\n",
        "    plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
        "    plt.title(\"Visualization of Enhanced CNN filters\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "visualize_filters(enhanced_cnn)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrRR7muurV9_",
        "outputId": "da6d92fb-bb0c-4c8a-f445-8a7b319ca9ff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:07<00:00, 73.1MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training VGG Transfer Learning...\n",
            "[1] loss: 1.3784062407358224\n",
            "[2] loss: 1.2861633935700292\n",
            "[3] loss: 1.2619935268788691\n",
            "[4] loss: 1.2578672875681192\n",
            "[5] loss: 1.2520083685207855\n",
            "Finished Training VGG Transfer Learning\n",
            "Accuracy of VGG Transfer Learning on the 10000 test images: 61%\n"
          ]
        }
      ],
      "source": [
        "# Transfer Learning with VGG\n",
        "vgg = models.vgg16(pretrained=True)\n",
        "for param in vgg.parameters():\n",
        "    param.requires_grad = False\n",
        "vgg.classifier[6] = nn.Linear(4096, 10)\n",
        "vgg_optimizer = optim.SGD(vgg.classifier[6].parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Training and Evaluating VGG\n",
        "train_and_evaluate(vgg, trainloader, testloader, vgg_optimizer, criterion, epochs=5, model_name=\"VGG Transfer Learning\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8oHKG2PzkF3",
        "outputId": "7d8513bb-324b-4bb7-a12a-d68abcae600b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Training VGG Transfer Learning...\n",
            "Epoch 1, Loss: 0.7356273643196086\n",
            "Epoch 2, Loss: 0.5968402636325573\n",
            "Epoch 3, Loss: 0.5693353946937625\n",
            "Epoch 4, Loss: 0.5569716241506054\n",
            "Epoch 5, Loss: 0.5482645163222042\n",
            "Finished Training VGG Transfer Learning\n",
            "Accuracy of VGG Transfer Learning on the 10000 test images: 82%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Set the device to GPU if available, otherwise fallback to CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define transformations for the CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize the images to fit VGG16\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Load the pretrained VGG16 model\n",
        "vgg = models.vgg16(pretrained=True)\n",
        "for param in vgg.parameters():\n",
        "    param.requires_grad = False  # Freeze the parameters in the feature extractor\n",
        "vgg.classifier[6] = nn.Linear(4096, 10)  # Adjust the classifier to CIFAR-10\n",
        "\n",
        "# Transfer the model to the appropriate device\n",
        "vgg.to(device)\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "vgg_optimizer = optim.SGD(vgg.classifier[6].parameters(), lr=0.001, momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Function to train and evaluate a model\n",
        "def train_and_evaluate(model, trainloader, testloader, optimizer, criterion, epochs, model_name):\n",
        "    print(f\"Training {model_name}...\")\n",
        "    model.to(device)\n",
        "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "        model.train()  # Set model to training mode\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(trainloader)}\")\n",
        "\n",
        "    print(f\"Finished Training {model_name}\")\n",
        "\n",
        "    # Evaluation phase\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Accuracy of {model_name} on the 10000 test images: {100 * correct // total}%')\n",
        "\n",
        "# Train and evaluate the VGG model\n",
        "train_and_evaluate(vgg, trainloader, testloader, vgg_optimizer, criterion, epochs=5, model_name=\"VGG Transfer Learning\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G90_FUMTjkZu",
        "outputId": "8aa94c3b-6b50-43ba-afcd-6fef4b8e4038"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Data preprocessing and augmentation techniques have been applied.\n"
          ]
        }
      ],
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.RandomAffine(0, shear=10, scale=(0.8, 1.2)),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Re-load the dataset with the new transformations\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
        "\n",
        "print(\"Data preprocessing and augmentation techniques have been applied.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGQnZYc4Q5Ak",
        "outputId": "74c5c8fa-b3dc-49ca-d38f-9f38d1548499"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Completed: LR=0.001, Optimizer=adam, Accuracy=77.41%\n",
            "Completed: LR=0.001, Optimizer=sgd, Accuracy=76.19%\n",
            "Completed: LR=0.0001, Optimizer=adam, Accuracy=75.63%\n",
            "Completed: LR=0.0001, Optimizer=sgd, Accuracy=60.42%\n",
            "LR: 0.001, Optimizer: adam, Accuracy: 77.41%\n",
            "LR: 0.001, Optimizer: sgd, Accuracy: 76.19%\n",
            "LR: 0.0001, Optimizer: adam, Accuracy: 75.63%\n",
            "LR: 0.0001, Optimizer: sgd, Accuracy: 60.42%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "\n",
        "\n",
        "# Define a function to train and evaluate the model\n",
        "def train_and_evaluate1(model, trainloader, testloader, optimizer, criterion, epochs=10):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Data loading\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "trainset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "testset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Hyperparameters to experiment with\n",
        "learning_rates = [0.001, 0.0001]\n",
        "optimizers = {\n",
        "    'adam': optim.Adam,\n",
        "    'sgd': lambda params, lr: optim.SGD(params, lr=lr, momentum=0.9)\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for opt_name, opt_constructor in optimizers.items():\n",
        "        model = EnhancedCNN()\n",
        "        optimizer = opt_constructor(model.parameters(), lr=lr)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        accuracy = train_and_evaluate1(model, trainloader, testloader, optimizer, criterion, epochs=10)\n",
        "        results.append((lr, opt_name, accuracy))\n",
        "        print(f\"Completed: LR={lr}, Optimizer={opt_name}, Accuracy={accuracy}%\")\n",
        "\n",
        "# Display all results\n",
        "for lr, opt_name, accuracy in results:\n",
        "    print(f\"LR: {lr}, Optimizer: {opt_name}, Accuracy: {accuracy}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Performance Comparison Across MLP, CNN, and VGG-based Models\n",
        "\n",
        "When evaluating the effectiveness of various neural network architectures on image classification tasks, we observe significant differences in performance metrics such as accuracy and loss. Below, we detail the outcomes for three models: MLP (Multilayer Perceptron), CNN (Convolutional Neural Network), and a VGG-based model utilizing transfer learning.\n",
        "\n",
        "## Test Set Performance\n",
        "\n",
        "- **MLP (Multilayer Perceptron)**: Achieved an accuracy of 52%. This model is the most basic of the three, lacking in spatial awareness and depth, which is reflected in its lower performance.\n",
        "  \n",
        "- **CNN (Convolutional Neural Network)**: Improved accuracy to 74%. CNNs are designed to process data in a grid pattern (like images), utilizing convolutional layers to capture the spatial hierarchy in images, leading to better feature extraction and performance.\n",
        "  \n",
        "- **VGG-based Model**: Delivered the highest accuracy at 82%. The VGG model, pre-trained on a vast dataset, brings the advantage of learned features that can be effectively transferred to new classification tasks, enhancing accuracy and potentially reducing training time.\n",
        "\n",
        "## Analysis of Performance Differences\n",
        "\n",
        "### MLP vs. CNN\n",
        "\n",
        "MLPs are fully connected networks where each input is processed independently, ignoring the spatial structure and correlation between adjacent pixels in images. This inherent limitation makes MLPs less suited for image-related tasks where the context and arrangement of pixels are crucial for understanding the content of the image.\n",
        "\n",
        "In contrast, CNNs explicitly take advantage of the image's spatial structure. Convolutional layers apply filters that capture local patterns such as edges, textures, and other features, pooling layers reduce dimensionality, and fully connected layers compile these features into high-level attributes. This structured approach allows CNNs to outperform MLPs in tasks involving images.\n",
        "\n",
        "### CNN vs. VGG-based Model\n",
        "\n",
        "While CNNs are powerful, designing an effective CNN from scratch requires careful consideration of architecture, depth, and hyperparameters. The VGG model, a type of CNN with a deep and well-established architecture, has been pre-trained on a large dataset (ImageNet), allowing it to learn a rich set of features. Utilizing such a model through transfer learning allows for leveraging these pre-learned features, which can be adapted to new tasks with relatively little additional training data.\n",
        "\n",
        "## Transfer Learning with VGG\n",
        "\n",
        "The use of the VGG model in a transfer learning context provides two main benefits: \n",
        "\n",
        "1. **Improved Performance**: The pre-learned features from the VGG model can significantly enhance the ability to classify images, even with datasets that are quite different from the original training data. This is evident in the higher accuracy achieved by the VGG-based model.\n",
        "\n",
        "2. **Training Time**: Since the model has already learned a substantial amount of useful features from a large and diverse dataset, the amount of time and data required to train the model on a new task is considerably reduced. This efficiency is particularly beneficial when computational resources are limited or when working with smaller datasets, but if dont backpropogate through all the layers training time would increase.\n",
        "\n",
        "In conclusion, the choice of model architecture is crucial when working with image data. The performance improvements seen with CNNs and VGG-based models underscore the importance of leveraging spatial structures and pre-learned features for more accurate and efficient image classification.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
